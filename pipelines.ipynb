{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split,StratifiedKFold # Model evaluation\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, OneHotEncoder, StandardScaler # Preprocessing\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet,  LassoLarsIC, RANSACRegressor, SGDRegressor, HuberRegressor, BayesianRidge # Linear models\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor  # Ensemble methods\n",
    "from xgboost import XGBRegressor, plot_importance # XGBoost\n",
    "from sklearn.svm import SVR, SVC, LinearSVC  # Support Vector Regression\n",
    "from sklearn.tree import DecisionTreeRegressor # Decision Tree Regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline, make_pipeline # Streaming pipelines\n",
    "from sklearn.decomposition import KernelPCA, PCA # Dimensionality reduction\n",
    "from sklearn.feature_selection import SelectFromModel # Dimensionality reduction\n",
    "from sklearn.model_selection import learning_curve, validation_curve, GridSearchCV # Model evaluation\n",
    "from sklearn.base import clone, BaseEstimator, TransformerMixin, RegressorMixin # Clone estimator\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import explained_variance_score, roc_auc_score, median_absolute_error, r2_score, mean_squared_error #To evaluate our model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, fbeta_score #To evaluate our model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/one_hot_train.csv\",index_col=0)\n",
    "test = pd.read_csv(\"../input/one_hot_test.csv\",index_col=0)\n",
    "y = pd.read_csv(\"../input/y.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target\n",
       "0            0\n",
       "1            0\n",
       "2            0\n",
       "3            0\n",
       "4            1\n",
       "...        ...\n",
       "299995       0\n",
       "299996       0\n",
       "299997       1\n",
       "299998       0\n",
       "299999       0\n",
       "\n",
       "[300000 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_Ridge: 0.880179 (+/- 0.001286)\n",
      "Scaled_Lasso: 0.879983 (+/- 0.001376)\n",
      "Scaled_Elastic: 0.880140 (+/- 0.001333)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/pipeline.py:346: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/pipeline.py:346: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/pipeline.py:346: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/pipeline.py:346: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/pipeline.py:346: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/pipeline.py:346: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/pipeline.py:346: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled_RF_reg: 0.880233 (+/- 0.001419)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/pipeline.py:346: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params_last_step)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#creates a list of models\n",
    "#create a pipeline by writing it out or using make_pipeline then you can test how well the model does using cross validation.\n",
    "#however in order to use it you have to do model.fit()\n",
    "##lasso/ridge alpha gives a standard for how many of the features should have linear coefficients of zero - higher alpha = higher standard\n",
    "pipelines = []\n",
    "seed = 5\n",
    "\n",
    "#https://www.statisticshowto.com/ridge-regression/\n",
    "#just enough biased variables so that it does not become too varianced \n",
    "#Ridge regression is a way to create a parsimonious model when the number of predictor variables in a set exceeds the number of observations, or when a data set has multicollinearity (correlations between predictor variables).\n",
    "pipelines.append(\n",
    "                (\"Scaled_Ridge\", \n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"Ridge\", Ridge(random_state=seed, alpha= 0.1, tol=0.1, solver='auto' ))]\n",
    "                 )))\n",
    "#https://www.statisticshowto.com/lasso-regression/ \n",
    "#Sparse using alpha decreases the amount of variables like ridge\n",
    "#Lasso, or Least Absolute Shrinkage and Selection Operator, is quite similar conceptually to ridge regression. It also adds a penalty for non-zero coefficients, but unlike ridge regression which penalizes sum of squared coefficients (the so-called L2 penalty), lasso penalizes the sum of their absolute values (L1 penalty). As a result, for high values of Î», many coefficients are exactly zeroed under lasso, which is never the case in ridge regression.\n",
    "pipelines.append(\n",
    "                (\"Scaled_Lasso\", \n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"Lasso\", Lasso(random_state=seed, alpha=.0005, tol=0.1))]\n",
    "                 )))\n",
    "#Elastic Net first emerged as a result of critique on lasso, whose variable selection can be too dependent on data and thus unstable. The solution is to combine the penalties of ridge regression and lasso to get the best of both worlds.\n",
    "pipelines.append(\n",
    "                (\"Scaled_Elastic\", \n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"Lasso\", ElasticNet(random_state=seed, alpha=.0005, tol=0.1))]\n",
    "                 )))\n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_RF_reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"RF\", RandomForestRegressor(random_state=seed))]\n",
    "                 )))\n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_ET_reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"ET\", ExtraTreesRegressor(random_state=seed))]\n",
    "                 )))\n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_BR_reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"BR\", BaggingRegressor(random_state=seed))]\n",
    "                 ))) \n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_Hub-Reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"Hub-Reg\", HuberRegressor())]\n",
    "                 ))) \n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_BayRidge\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"BR\", BayesianRidge())]\n",
    "                 ))) \n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_XGB_reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"XGBR\", XGBRegressor(seed=seed, n_estimators=300))]\n",
    "                 ))) \n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_DT_reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"DT_reg\", DecisionTreeRegressor())]\n",
    "                 ))) \n",
    "\n",
    "\"\"\"pipelines.append(\n",
    "                (\"Scaled_SVR\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"SVR\",  SVR(kernel='linear', C=1e3, degree=2))]\n",
    "                 )))\"\"\"\n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_KNN_reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"KNN_reg\", KNeighborsRegressor())]\n",
    "                 )))\n",
    "pipelines.append(\n",
    "                (\"Scaled_ADA-Reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"ADA-reg\", AdaBoostRegressor())\n",
    "                 ]))) \n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_Gboost-Reg\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"GBoost-Reg\", GradientBoostingRegressor())]\n",
    "                 )))\n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_RFR_PCA\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"PCA\", PCA(n_components=3)),\n",
    "                     (\"XGB\", RandomForestRegressor())]\n",
    "                 )))\n",
    "\n",
    "pipelines.append(\n",
    "                (\"Scaled_XGBR_PCA\",\n",
    "                 Pipeline([\n",
    "                     (\"Scaler\", StandardScaler()),\n",
    "                     (\"PCA\", PCA(n_components=3)),\n",
    "                     (\"XGB\", XGBRegressor())]\n",
    "                 )))\n",
    "\n",
    "#'neg_mean_absolute_error', 'neg_mean_squared_error','r2'\n",
    "scoring = 'roc_auc'\n",
    "n_folds = 7\n",
    "\n",
    "results, names  = [], [] \n",
    "\n",
    "for name, model  in pipelines:\n",
    "    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    cv_results = cross_val_score(model, train, y, cv= kfold,\n",
    "                                 scoring=scoring, n_jobs=1)    \n",
    "    names.append(name)\n",
    "    results.append(cv_results)    \n",
    "    msg = \"%s: %f (+/- %f)\" % (name, cv_results.mean(),  cv_results.std())\n",
    "    print(msg)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['explained_variance', 'r2', 'max_error', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error', 'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'top_k_accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'neg_brier_score', 'adjusted_rand_score', 'rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.metrics.SCORERS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
