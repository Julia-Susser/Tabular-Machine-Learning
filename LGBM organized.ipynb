{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split,StratifiedKFold # Model evaluation\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler, OneHotEncoder, StandardScaler # Preprocessing\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet,  LassoLarsIC, RANSACRegressor, SGDRegressor, HuberRegressor, BayesianRidge # Linear models\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor  # Ensemble methods\n",
    "from xgboost import XGBRegressor, plot_importance # XGBoost\n",
    "from sklearn.svm import SVR, SVC, LinearSVC  # Support Vector Regression\n",
    "from sklearn.tree import DecisionTreeRegressor # Decision Tree Regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline, make_pipeline # Streaming pipelines\n",
    "from sklearn.decomposition import KernelPCA, PCA # Dimensionality reduction\n",
    "from sklearn.feature_selection import SelectFromModel # Dimensionality reduction\n",
    "from sklearn.model_selection import learning_curve, validation_curve, GridSearchCV # Model evaluation\n",
    "from sklearn.base import clone, BaseEstimator, TransformerMixin, RegressorMixin # Clone estimator\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import explained_variance_score, roc_auc_score, median_absolute_error, r2_score, mean_squared_error #To evaluate our model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, fbeta_score #To evaluate our model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split \n",
    "from scipy.stats import norm, skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "testID = test.id\n",
    "categorical_cols = train.select_dtypes(\"object\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.target\n",
    "train = train.drop([\"target\"],axis=1)\n",
    "features = pd.concat([train,test])\n",
    "features = features.drop([\"id\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11 skewed numerical features to Box Cox transform\n"
     ]
    }
   ],
   "source": [
    "def skew(features):\n",
    "    from scipy.stats import norm, skew\n",
    "    numeric_feats = features.dtypes[features.dtypes != \"object\"].index\n",
    "\n",
    "    # Check the skew of all numerical features\n",
    "    skewed_feats = features[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "    skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "\n",
    "\n",
    "    skewness = skewness[abs(skewness) > 0.75]\n",
    "    print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "\n",
    "    from scipy.special import boxcox1p\n",
    "    skewed_features = skewness.index\n",
    "    lam = 0.15\n",
    "    for feat in skewed_features:\n",
    "        features[feat] = boxcox1p(features[feat], lam)\n",
    "    return features\n",
    "\n",
    "if skew:\n",
    "    features = skew(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = True\n",
    "label_encoder = False\n",
    "overfit = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "if one_hot:\n",
    "    features = pd.get_dummies(features)\n",
    "    if overfit:\n",
    "        x = features == 0\n",
    "        x = (x.sum()/len(features)>.99).to_frame()\n",
    "        drop_cols = x[x.iloc[:,0]].index\n",
    "        features = features.drop(drop_cols,axis=1)\n",
    "\n",
    "    \n",
    "if label_encoder:\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    for col in categorical_cols:\n",
    "        features[col] = le.fit_transform(features[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = features[:len(y)]\n",
    "test = features[len(y):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression works like linear regression but shifts the line forward depending on where the switch is until it maximizes likelihood\n",
    "\n",
    "pipelines = {\n",
    "    \"logistic_regression\": Pipeline([\n",
    "    ('Scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression()),\n",
    "    ]),\n",
    "     \n",
    "    \n",
    "    \"xgb\": Pipeline([\n",
    "    ('Scaler', StandardScaler()),\n",
    "    ('classifier', XGBRegressor(objective ='reg:linear', \n",
    "                  n_estimators = 10, seed = 123)),\n",
    "    ]),\n",
    "    \n",
    "    \"xgb2\": xgb.XGBClassifier(n_estimators=5,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.5,\n",
    "    seed=123)\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.8942635164251782\n",
      "2 0.8960967723185278\n",
      "3 0.894482208007798\n",
      "4 0.8956907416756221\n",
      "5 0.8965596568800058\n"
     ]
    }
   ],
   "source": [
    "params={'metric': 'auc', 'reg_alpha': 6.010538011450937, 'reg_lambda': 0.031702113663443346, 'colsample_bytree': 0.27,\n",
    "   'subsample': 0.6, 'learning_rate': 0.05, 'max_depth': 100, 'num_leaves': 100, 'min_child_samples': 216,\n",
    "   'cat_smooth': 87, 'random_state': 48,'n_estimators': 20000}\n",
    "preds = np.zeros(test.shape[0])        \n",
    "kf = StratifiedKFold(n_splits=5,random_state=48,shuffle=True) #As we can see the data is unbalanced that's why I'll use StratifiedKFold to split data: Don't want all zeros in a split                 \n",
    "auc=[]   # list contains AUC for each fold  \n",
    "n=0   \n",
    "for trn_idx, test_idx in kf.split(train,y):\n",
    "    X_tr,X_val=train.iloc[trn_idx],train.iloc[test_idx]\n",
    "    y_tr,y_val=y.iloc[trn_idx],y.iloc[test_idx]\n",
    "    model = LGBMClassifier(**params) \n",
    "    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False) \n",
    "    preds+=model.predict_proba(test)[:, 1]/kf.n_splits \n",
    "    auc.append(roc_auc_score(y_val, model.predict_proba(X_val)[:, 1])) \n",
    "    print(n+1,auc[n])                                                                                       \n",
    "    n+=1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({\"id\":testID,\"target\":preds})\n",
    "sub.to_csv(\"../output/LGB_sub2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0.115566\n",
       "1         0.447129\n",
       "2         0.010611\n",
       "3         0.239745\n",
       "4         0.101847\n",
       "            ...   \n",
       "199995    0.896518\n",
       "199996    0.049339\n",
       "199997    0.701568\n",
       "199998    0.115219\n",
       "199999    0.526594\n",
       "Name: target, Length: 200000, dtype: float64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = pd.read_csv(\"../output/LGB_sub.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
